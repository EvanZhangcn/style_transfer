"""
åŸºäºConvNeXtçš„ç°ä»£ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»å®ç°
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
from torchvision.models import convnext_base, ConvNeXt_Base_Weights
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import os
import time
import warnings
warnings.filterwarnings('ignore')


class ConvNeXtFeatureExtractor(nn.Module):
    """
    ConvNeXtç‰¹å¾æå–å™¨
    """
    
    def __init__(self, device='cpu'):
        super(ConvNeXtFeatureExtractor, self).__init__()
        self.device = device
        
        # åŠ è½½é¢„è®­ç»ƒçš„ConvNeXtæ¨¡å‹
        weights = ConvNeXt_Base_Weights.IMAGENET1K_V1
        self.model = convnext_base(weights=weights).to(device)
        self.model.eval()
        
        # å†»ç»“æ¨¡å‹å‚æ•°
        for param in self.model.parameters():
            param.requires_grad_(False)
        
        # ConvNeXtçš„ç‰¹å¾å±‚å®šä¹‰
        # ConvNeXtåˆ†ä¸º4ä¸ªstageï¼Œæ¯ä¸ªstageåŒ…å«å¤šä¸ªblock
        self.feature_layers = {
            'stage1_block0': 'features.1.0',      # æ—©æœŸç‰¹å¾ï¼Œçº¹ç†
            'stage1_block2': 'features.1.2',      # æ—©æœŸç‰¹å¾ï¼Œçº¹ç†
            'stage2_block0': 'features.3.0',      # ä¸­å±‚ç‰¹å¾ï¼Œå±€éƒ¨æ¨¡å¼
            'stage2_block2': 'features.3.2',      # ä¸­å±‚ç‰¹å¾ï¼Œå±€éƒ¨æ¨¡å¼
            'stage3_block0': 'features.5.0',      # é«˜å±‚ç‰¹å¾ï¼Œå†…å®¹
            'stage3_block9': 'features.5.9',      # é«˜å±‚ç‰¹å¾ï¼Œå†…å®¹
            'stage4_block0': 'features.7.0',      # æœ€é«˜å±‚ç‰¹å¾ï¼Œè¯­ä¹‰
            'stage4_block2': 'features.7.2',      # æœ€é«˜å±‚ç‰¹å¾ï¼Œè¯­ä¹‰
        }
        
        # é£æ ¼å±‚ï¼šå¤šä¸ªå±‚æ¬¡çš„ç‰¹å¾ç”¨äºæ•è·ä¸åŒå°ºåº¦çš„çº¹ç†
        self.style_layers = [
            'stage1_block0',  # ä½å±‚çº¹ç†
            'stage1_block2',  # ä½å±‚çº¹ç†
            'stage2_block0',  # ä¸­å±‚æ¨¡å¼
            'stage2_block2',  # ä¸­å±‚æ¨¡å¼
            'stage3_block0',  # é«˜å±‚æ¨¡å¼
        ]
        
        # å†…å®¹å±‚ï¼šé«˜å±‚ç‰¹å¾ç”¨äºä¿æŒè¯­ä¹‰å†…å®¹
        self.content_layers = [
            'stage3_block9',  # ä¸»è¦å†…å®¹ç‰¹å¾
        ]
        
        self.normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)
        self.normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)
    
    def get_features(self, x, layers):
        """æå–æŒ‡å®šå±‚çš„ç‰¹å¾"""
        features = {}
        
        # ConvNeXtå‰å‘ä¼ æ’­å¹¶æå–ä¸­é—´ç‰¹å¾
        x = self.model.features[0](x)  # stem layer
        
        # Stage 1
        for i, layer in enumerate(self.model.features[1]):
            x = layer(x)
            layer_name = f'stage1_block{i}'
            if layer_name in layers:
                features[layer_name] = x
        
        x = self.model.features[2](x)  # downsample
        
        # Stage 2
        for i, layer in enumerate(self.model.features[3]):
            x = layer(x)
            layer_name = f'stage2_block{i}'
            if layer_name in layers:
                features[layer_name] = x
        
        x = self.model.features[4](x)  # downsample
        
        # Stage 3
        for i, layer in enumerate(self.model.features[5]):
            x = layer(x)
            layer_name = f'stage3_block{i}'
            if layer_name in layers:
                features[layer_name] = x
        
        x = self.model.features[6](x)  # downsample
        
        # Stage 4
        for i, layer in enumerate(self.model.features[7]):
            x = layer(x)
            layer_name = f'stage4_block{i}'
            if layer_name in layers:
                features[layer_name] = x
        
        return features


class ModernStyleTransfer:
    """
    åŸºäºConvNeXtçš„ç°ä»£é£æ ¼è¿ç§»ç±»
    """
    
    def __init__(self, device='cpu', max_size=512):
        self.device = device
        self.max_size = max_size
        self.feature_extractor = ConvNeXtFeatureExtractor(device)
        
        print(f"ğŸš€ Modern Style Transfer åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“± è®¾å¤‡: {device}")
        print(f"ğŸ–¼ï¸  æœ€å¤§å›¾åƒå°ºå¯¸: {max_size}")
        print(f"ğŸ§  ç‰¹å¾æå–å™¨: ConvNeXt-Base (2022)")
        
    def load_image(self, image_path, size=None):
        """åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ"""
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        
        image = Image.open(image_path).convert('RGB')
        
        if size is None:
            size = min(self.max_size, max(image.size))
        
        # ä¿æŒå®½é«˜æ¯”çš„resize
        transform = transforms.Compose([
            transforms.Resize(size),
            transforms.ToTensor(),
        ])
        
        image = transform(image).unsqueeze(0)
        return image.to(self.device)
    
    def save_image(self, tensor, path):
        """ä¿å­˜å›¾åƒ"""
        image = tensor.cpu().clone().detach()
        image = image.squeeze(0)
        image = torch.clamp(image, 0, 1)
        
        transform = transforms.ToPILImage()
        image = transform(image)
        
        os.makedirs(os.path.dirname(path), exist_ok=True)
        image.save(path)
    
    def gram_matrix(self, tensor):
        """
        è®¡ç®—GramçŸ©é˜µï¼Œç”¨äºæ•è·é£æ ¼ç‰¹å¾
        GramçŸ©é˜µèƒ½å¤Ÿæ•è·ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»£è¡¨çº¹ç†ä¿¡æ¯
        """
        _, d, h, w = tensor.size()
        tensor = tensor.view(d, h * w)
        gram = torch.mm(tensor, tensor.t())
        return gram
    
    def content_loss(self, target_features, content_features):
        """è®¡ç®—å†…å®¹æŸå¤±"""
        loss = 0
        for layer in self.feature_extractor.content_layers:
            target_feature = target_features[layer]
            content_feature = content_features[layer]
            loss += torch.mean((target_feature - content_feature) ** 2)
        return loss
    
    def style_loss(self, target_features, style_grams):
        """è®¡ç®—é£æ ¼æŸå¤±"""
        loss = 0
        for layer in self.feature_extractor.style_layers:
            target_feature = target_features[layer]
            target_gram = self.gram_matrix(target_feature)
            style_gram = style_grams[layer]
            
            # å½’ä¸€åŒ–GramçŸ©é˜µ
            _, d, h, w = target_feature.shape
            layer_loss = torch.mean((target_gram - style_gram) ** 2)
            layer_loss = layer_loss / (d * h * w)
            
            loss += layer_loss
        return loss
    
    def total_variation_loss(self, image):
        """
        æ€»å˜åˆ†æŸå¤±ï¼Œç”¨äºå›¾åƒå¹³æ»‘
        å‡å°‘å™ªå£°ï¼Œä½¿ç”Ÿæˆçš„å›¾åƒæ›´è‡ªç„¶
        """
        tv_h = torch.mean(torch.abs(image[:, :, 1:, :] - image[:, :, :-1, :]))
        tv_w = torch.mean(torch.abs(image[:, :, :, 1:] - image[:, :, :, :-1]))
        return tv_h + tv_w
    
    def transfer_style(self, content_path, style_path, output_path=None,
                      num_steps=1000, style_weight=1e6, content_weight=1,
                      tv_weight=1e-3, lr=0.01, optimizer_type='adam',
                      save_every=100, show_progress=True):
        """
        æ‰§è¡Œé£æ ¼è¿ç§»
        
        Args:
            content_path: å†…å®¹å›¾åƒè·¯å¾„
            style_path: é£æ ¼å›¾åƒè·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„
            num_steps: ä¼˜åŒ–æ­¥æ•°
            style_weight: é£æ ¼æŸå¤±æƒé‡
            content_weight: å†…å®¹æŸå¤±æƒé‡
            tv_weight: æ€»å˜åˆ†æŸå¤±æƒé‡
            lr: å­¦ä¹ ç‡
            optimizer_type: ä¼˜åŒ–å™¨ç±»å‹ ('adam', 'lbfgs')
            save_every: æ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸­é—´ç»“æœ
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        """
        
        print("ğŸ¨ å¼€å§‹ç°ä»£é£æ ¼è¿ç§»...")
        start_time = time.time()
        
        # åŠ è½½å›¾åƒ
        content_img = self.load_image(content_path)
        style_img = self.load_image(style_path)
        
        print(f"ğŸ“¸ å†…å®¹å›¾åƒå°ºå¯¸: {content_img.shape}")
        print(f"ğŸ­ é£æ ¼å›¾åƒå°ºå¯¸: {style_img.shape}")
        
        # åˆå§‹åŒ–ç›®æ ‡å›¾åƒ
        target_img = content_img.clone().requires_grad_(True)
        
        # æå–ç‰¹å¾
        all_layers = self.feature_extractor.content_layers + self.feature_extractor.style_layers
        
        content_features = self.feature_extractor.get_features(content_img, all_layers)
        style_features = self.feature_extractor.get_features(style_img, all_layers)
        
        # é¢„è®¡ç®—é£æ ¼ç‰¹å¾çš„GramçŸ©é˜µ
        style_grams = {}
        for layer in self.feature_extractor.style_layers:
            style_grams[layer] = self.gram_matrix(style_features[layer])
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        if optimizer_type.lower() == 'lbfgs':
            optimizer = optim.LBFGS([target_img], lr=lr, max_iter=20)
        else:
            optimizer = optim.Adam([target_img], lr=lr)
        
        print(f"âš™ï¸ ä¼˜åŒ–å™¨: {optimizer.__class__.__name__}")
        print(f"ğŸ“Š æƒé‡è®¾ç½® - é£æ ¼: {style_weight}, å†…å®¹: {content_weight}, TV: {tv_weight}")
        
        # æŸå¤±è®°å½•
        losses = {
            'total': [],
            'content': [],
            'style': [],
            'tv': []
        }
        
        def closure():
            # æ¸…é™¤æ¢¯åº¦
            optimizer.zero_grad()
            
            # æå–ç›®æ ‡å›¾åƒç‰¹å¾
            target_features = self.feature_extractor.get_features(target_img, all_layers)
            
            # è®¡ç®—å„é¡¹æŸå¤±
            c_loss = self.content_loss(target_features, content_features)
            s_loss = self.style_loss(target_features, style_grams)
            tv_loss = self.total_variation_loss(target_img)
            
            # åŠ æƒæ€»æŸå¤±
            total_loss = (content_weight * c_loss + 
                         style_weight * s_loss + 
                         tv_weight * tv_loss)
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # è®°å½•æŸå¤±
            losses['total'].append(total_loss.item())
            losses['content'].append(c_loss.item())
            losses['style'].append(s_loss.item())
            losses['tv'].append(tv_loss.item())
            
            return total_loss
        
        # è®­ç»ƒå¾ªç¯
        for step in range(num_steps):
            if optimizer_type.lower() == 'lbfgs':
                optimizer.step(closure)
            else:
                loss = closure()
                optimizer.step()
            
            # é™åˆ¶åƒç´ å€¼èŒƒå›´
            with torch.no_grad():
                target_img.clamp_(0, 1)
            
            # æ˜¾ç¤ºè¿›åº¦
            if show_progress and step % 50 == 0:
                current_loss = losses['total'][-1]
                elapsed = time.time() - start_time
                print(f"æ­¥éª¤ {step:4d}/{num_steps} | "
                      f"æ€»æŸå¤±: {current_loss:.2e} | "
                      f"è€—æ—¶: {elapsed:.1f}s")
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            if save_every > 0 and step > 0 and step % save_every == 0 and output_path:
                intermediate_path = output_path.replace('.', f'_step_{step}.')
                self.save_image(target_img, intermediate_path)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        if output_path:
            self.save_image(target_img, output_path)
            print(f"âœ… é£æ ¼è¿ç§»å®Œæˆï¼ç»“æœä¿å­˜è‡³: {output_path}")
        
        total_time = time.time() - start_time
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f}ç§’")
        
        return target_img, losses
    
    def plot_losses(self, losses):
        """ç»˜åˆ¶æŸå¤±æ›²çº¿"""
        plt.figure(figsize=(12, 8))
        
        plt.subplot(2, 2, 1)
        plt.plot(losses['total'])
        plt.title('Total Loss')
        plt.ylabel('Loss')
        
        plt.subplot(2, 2, 2)
        plt.plot(losses['content'])
        plt.title('Content Loss')
        plt.ylabel('Loss')
        
        plt.subplot(2, 2, 3)
        plt.plot(losses['style'])
        plt.title('Style Loss')
        plt.ylabel('Loss')
        
        plt.subplot(2, 2, 4)
        plt.plot(losses['tv'])
        plt.title('Total Variation Loss')
        plt.ylabel('Loss')
        
        plt.tight_layout()
        plt.show()


def main():
    """
    ç°ä»£é£æ ¼è¿ç§»æ¼”ç¤º
    """
    # æ£€æµ‹è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    if device.type == 'cuda':
        print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    # åˆ›å»ºç°ä»£é£æ ¼è¿ç§»å¯¹è±¡
    style_transfer = ModernStyleTransfer(device=device, max_size=512)
    
    # è®¾ç½®è·¯å¾„
    content_path = "images/content/photo.jpg"
    style_path = "images/style/artwork.jpg"
    output_path = "results/modern_result.jpg"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(content_path):
        print(f"âŒ å†…å®¹å›¾åƒä¸å­˜åœ¨: {content_path}")
        print("è¯·å°†å†…å®¹å›¾åƒæ”¾åœ¨ images/content/ ç›®å½•ä¸‹")
        return
    
    if not os.path.exists(style_path):
        print(f"âŒ é£æ ¼å›¾åƒä¸å­˜åœ¨: {style_path}")
        print("è¯·å°†é£æ ¼å›¾åƒæ”¾åœ¨ images/style/ ç›®å½•ä¸‹")
        return
    
    try:
        # æ‰§è¡Œé£æ ¼è¿ç§»
        result_img, losses = style_transfer.transfer_style(
            content_path=content_path,
            style_path=style_path,
            output_path=output_path,
            num_steps=800,
            style_weight=1e6,
            content_weight=1,
            tv_weight=1e-3,
            lr=0.01,
            optimizer_type='adam',
            save_every=200
        )
        
        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        style_transfer.plot_losses(losses)
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
